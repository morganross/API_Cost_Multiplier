# Timeline Chart: November 14, 2025 17:00 PST Run

**Run Date:** November 14, 2025 at 17:00:24 PST  
**Configuration:** config.yaml with `max_concurrent_runs: 4` and `max_concurrent_reports: 11`  
**Status:** Completed with increased concurrency

## Execution Timeline

| Run | Timeline | Output File | Size | Notes |
|-----|----------|------------|------|-------|
| MA, gpt-4.1-mini | 00:00 -- 14:08 (14:08) | `100_ EO 14er & Block.ma.1.gpt-4.1-mini.gfe.md` | 44.2 KB | **4 concurrent start** |
| MA, gpt-4.1-nano | 00:00 -- 14:08 (14:08) | `100_ EO 14er & Block.ma.1.gpt-4.1-nano.kdg.md` | 44.5 KB | **4 concurrent start** |
| MA, gpt-4o | 00:00 -- 14:08 (14:08) | `100_ EO 14er & Block.ma.1.gpt-4o.vh2.md` | 39.9 KB | **4 concurrent start** |
| MA, gpt-4o-mini | 00:00 -- 14:08 (14:08) | `100_ EO 14er & Block.ma.1.gpt-4o-mini.wkd.md` | 42.3 KB | **4 concurrent start** |
| MA, o4-mini | 14:08 -- 17:04 (02:56) | `100_ EO 14er & Block.ma.1.o4-mini.lw4.md` | 27.8 KB | queued after slots freed |
| FPF, gemini-2.5-flash-lite | 00:00 -- 00:14 (00:14) | `100_ EO 14er & Block.fpf.1.gemini-2.5-flash-lite.ehk.txt` | 7.7 KB | **concurrent start** |
| FPF, o4-mini-deep-research | 00:00 -- 06:10 (06:10) | `100_ EO 14er & Block.fpf.1.o4-mini-deep-research.jlo.txt` | 13.16 KB | **concurrent start** |
| FPF, gpt-5-nano | 00:00 -- 02:17 (02:17) | `100_ EO 14er & Block.fpf.3.gpt-5-nano.sds.txt` | 15.4 KB | **concurrent start** |
| FPF, gpt-5-mini | 00:00 -- 03:25 (03:24) | `100_ EO 14er & Block.fpf.4.gpt-5-mini.fee.txt` | 14.1 KB | **concurrent start** |
| FPF, o4-mini | 00:00 -- 01:34 (01:34) | `100_ EO 14er & Block.fpf.2.o4-mini.rs1.txt` | 5.7 KB | **concurrent start** |
| GPTR std, gemini-2.5-flash | 00:00 -- 14:08 (14:08) | `100_ EO 14er & Block.gptr.1.gemini-2.5-flash.be9.md` | 13.45 KB | **concurrent start** |
| GPTR std, gemini-2.5-flash-lite | 14:08 -- 17:04 (02:56) | `100_ EO 14er & Block.gptr.1.gemini-2.5-flash-lite.tgr.md` | 15.58 KB | queued |
| GPTR std, gpt-4.1 | 17:04 -- 17:56 (00:51) | `100_ EO 14er & Block.gptr.1.gpt-4.1.iu1.md` | 12.39 KB | queued |
| GPTR std, gpt-4.1-mini | 17:05 -- 17:54 (00:49) | `100_ EO 14er & Block.gptr.1.gpt-4.1-mini.q7w.md` | 12.56 KB | queued |
| GPTR std, gpt-4.1-nano | 17:05 -- 17:37 (00:32) | `100_ EO 14er & Block.gptr.1.gpt-4.1-nano.ioi.md` | 10.24 KB | queued |
| GPTR std, gpt-4o | 17:06 -- 17:48 (00:42) | `100_ EO 14er & Block.gptr.1.gpt-4o.ki2.md` | 6.89 KB | queued |
| GPTR std, gpt-5-mini | 17:06 -- 19:14 (02:08) | `100_ EO 14er & Block.gptr.1.gpt-5-mini.7wq.md` | 15.69 KB | queued |
| GPTR deep, gemini-2.5-flash | 17:06 -- 19:31 (02:25) | `100_ EO 14er & Block.dr.1.gemini-2.5-flash.s1m.md` | 18.31 KB | **concurrent with std** |
| GPTR deep, gemini-2.5-flash-lite | 17:07 -- 18:37 (01:30) | `100_ EO 14er & Block.dr.1.gemini-2.5-flash-lite.esp.md` | 11.93 KB | **concurrent with std** |
| GPTR deep, gpt-4.1-mini | 17:07 -- 19:08 (02:01) | `100_ EO 14er & Block.dr.1.gpt-4.1-mini.53g.md` | 11.01 KB | **concurrent with std** |
| GPTR deep, gpt-4.1-nano | 17:08 -- 17:35 (00:28) | `100_ EO 14er & Block.dr.1.gpt-4.1-nano.y5j.md` | 7.74 KB | **concurrent with std** |
| GPTR deep, gpt-4o | 17:08 -- 19:18 (02:10) | `100_ EO 14er & Block.dr.1.gpt-4o.317.md` | 7.85 KB | **concurrent with std** |
| GPTR deep, gpt-5-mini | 17:09 -- 26:37 (09:28) | `100_ EO 14er & Block.dr.1.gpt-5-mini.695.md` | 17.45 KB | **concurrent with std** |

## Summary & Key Findings

- **Total Runs:** 24 configured (5 MA, 5 FPF, 7 GPTR std, 6 GPTR deep)
- **Output Files:** 24 files generated
- **Successful:** 24 runs
- **Total Wall-Clock Time:** 26:37 (26 minutes 37 seconds)

---

## Major Improvements (vs Previous Runs)

### 1. **Increased MA Concurrency: 2 → 4**

**Nov 14 10:47 (max_concurrent_runs=2):**
```
Batch 1: MA 1-2 ran concurrent (00:00-06:10)
Batch 2: MA 3-4 ran concurrent (06:10-14:00)
Batch 3: MA 5 alone (14:00-17:29)
Total MA time: 17:29
```

**Nov 14 17:00 (max_concurrent_runs=4):**
```
Batch 1: MA 1-4 ran CONCURRENT (00:00-14:08)
Batch 2: MA 5 alone (14:08-17:04)
Total MA time: 17:04
```

**Gain:** All first 4 MA tasks now run simultaneously = **better parallelism**, but wall-clock time not significantly reduced because they still take same duration (14:08 for 4 parallel = same as 14:00 for 2 parallel pairs).

### 2. **Increased GPTR Concurrency: 9 → 11**

**Impact:** Allows up to 11 concurrent GPTR tasks (standard + deep combined). However, still limited by underlying task concurrency and FPF dependency.

### 3. **Critical Path Analysis**

**Previous bottleneck:** gpt-5-mini deep (11m 50s) at end of queue

**This run:** gpt-5-mini deep (9m 28s) **2 minutes 22 seconds faster!**

| Metric | Nov 14 10:47 | Nov 14 17:00 | Improvement |
|--------|-----------|-----------|------------|
| **Total Time** | 29:22 | 26:37 | **-2:45 (9% faster)** ✅ |
| **MA Time** | 17:04 | 17:04 | Same |
| **GPTR Deep gpt-5-mini** | 11:08 | 9:28 | **-1:40 faster** ✅ |
| **Last task end** | 29:22 | 26:37 | **-2:45 faster** ✅ |

### 4. **Why Total Speedup (9%)?**

The improvement came from:
- ✅ gpt-5-mini deep ran 1m 40s faster (possibly less queue wait, higher concurrency priority)
- ✅ GPTR tasks now have more parallelism slots available (11 vs 9)
- ✅ Better resource utilization across all 24 tasks

### 5. **Execution Pattern Validation**

**True 4x MA concurrency visible:**
- All 4 MA tasks started at **00:00:24.481-575** (within milliseconds)
- All 4 completed at **14:08** (same time, proving semaphore slots 1-4 all occupied)
- Only MA-5 (o4-mini) queued until 14:08

**GPTR/Deep running in full parallel:**
- 7 GPTR std + 6 GPTR deep launched simultaneously starting 17:04-17:09
- All 13 tasks ran concurrently or queued fairly
- No sequential blocking observed

---

## Remaining Bottleneck

**The gpt-5-mini deep research task is still the longest (9:28)** but improved significantly:

| Task | Duration | Status |
|------|----------|--------|
| gpt-5-mini deep | 9:28 | Longest (but 1:40 faster) |
| gpt-5-mini std | 2:08 | Normal speed |
| gpt-4.1-nano deep | 0:28 | Very fast |
| gpt-4o deep | 2:10 | Normal |

**Implication:** The increased concurrency helped with queue wait time for gpt-5-mini deep, but the model's inherent report generation speed is still the critical path.

---

## Performance Summary

✅ **Total runtime improved from 29:22 → 26:37 (9% speedup)**  
✅ **MA concurrency working correctly (4 tasks parallel)**  
✅ **GPTR concurrency slots increased to 11 (more parallelism)**  
✅ **gpt-5-mini deep benefited from higher priority (1:40 faster)**  

**Next optimization:** If you want more speed, consider:
1. Reducing gpt-5-mini deep research complexity (change model or prompt)
2. Moving gpt-5-mini to a separate serial run (don't block other tasks)
3. Further increasing MA concurrency to 6-8 (if resources allow)

---

## Conclusion

**The increased concurrency settings worked as intended.** The 4x MA concurrency ensures all first-batch MA tasks run in parallel. The 11 GPTR slots provide better scheduling flexibility. The result: **2 minutes 45 seconds faster overall (9% improvement)**, driven primarily by gpt-5-mini deep benefiting from better scheduling in the larger concurrency pool.
