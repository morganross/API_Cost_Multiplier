# GPT-Researcher Run Issues Report

Generation date: 2025-10-04
Generated by: automated diagnostics (assistant)
Files consulted:
- api_cost_multiplier/config.yaml
- api_cost_multiplier/test/generate_run_results.csv
- api_cost_multiplier/runner.py
- api_cost_multiplier/functions/gpt_researcher_client.py
- api_cost_multiplier/functions/gptr_subprocess.py
- temp_gpt_researcher_reports/* (sample files)
- api_cost_multiplier/test/mdinputs/commerce/Census Bureau.md

---

## Executive summary

This report documents the issues observed when running the current pipeline (generate.py / runner.py) that orchestrates GPT-Researcher (gpt-r), Multi-Agent (MA), and FilePromptForge (FPF). The majority of failures originate inside gpt-researcher (scrapers/LLM pipeline and provider parameter handling). The orchestration layer (api_cost_multiplier, "ACM") surfaced the failures, performed limited mitigation (streaming child output and setting UTF-8 env vars), and attempted to capture failure artifacts, but a small set of remaining gaps cause runs to fail or produce unclear failures.

High-level outcomes from the last run:
- Successful runs: openai:gpt-5 (gptr), openai:o3 (dr)
- Failures: gpt-5-mini, gpt-5-nano (multiple run types) due to encoding errors, empty report output, and provider parameter validation.

---

## Environment & run configuration (relevant excerpts)

From `api_cost_multiplier/config.yaml`:
- input_folder: C:\dev\silky\api_cost_multiplier\test\mdinputs
- output_folder: C:\dev\silky\api_cost_multiplier\test\mdoutputs
- instructions_file: C:\dev\silky\api_cost_multiplier\test\instructions.txt
- one_file_only: true
- iterations_default: 1
- runs (configured sequence):
  - gptr openai gpt-5
  - gptr openai gpt-5-mini
  - gptr openai gpt-5-nano
  - dr openai gpt-5-mini
  - dr openai gpt-5-nano
  - dr openai o3

Runner behavior:
- Per-run: patches gpt-researcher default config (default.py) with provider:model, then spawns `functions/gptr_subprocess.py` as a separate Python process to run gpt-researcher using the prompt file.
- Runner sets child env defaults:
  - PYTHONIOENCODING = utf-8
  - PYTHONUTF8 = 1
- Child is spawned with stdout/stderr streaming so live logs are visible in the parent process.

Timeouts and artifacts:
- Programmatic runs use a 600s timeout (GPTR_TIMEOUT_SECONDS).
- When no outputs are produced, runner writes `.gptr.failed.json` or `.dr.failed.json` artifacts in output folder.

---

## Observed runs (from api_cost_multiplier/test/generate_run_results.csv)

run_index, type, provider, model, outcome (summary)
- 0 — gptr — openai — gpt-5 — success  
- 1 — gptr — openai — gpt-5-mini — error ('charmap' codec encoding issue)  
- 2 — gptr — openai — gpt-5-nano — error ('charmap' codec encoding issue)  
- 3 — dr   — openai — gpt-5-mini — error (GPTResearcher.write_report() returned no content)  
- 4 — dr   — openai — gpt-5-nano — error (Unsupported value: 'temperature' not allowed)  
- 5 — dr   — openai — o3 — success

Sample temp report produced for run 0 (openai:gpt-5):
- temp_gpt_researcher_reports/research_report_38183dd4-7194-44d7-a732-ec78918ec36b.md
  - Contains a full topical research report (example excerpt saved in temp_gpt_researcher_reports).

---

## Detailed issues, evidence, and prior attempts

Each issue below is documented with observed evidence, the likely root cause, and previous attempts (if any) to mitigate.

### 1) Encoding / 'charmap' codec errors
- Symptom / evidence:
  - CSV rows for runs 1 and 2 record errors: "gpt-researcher failed: 'charmap' codec can't encode characters in position 0-1: character maps to <undefined>"
  - These errors appeared during gpt-researcher scraping/parsing stages (observed in runtime logs).
- Root cause:
  - On Windows, the default stdio encoding can be cp1252 (or similar). When gpt-researcher reads external content (web pages, PDFs, or binary streams) and attempts to log or re-encode bytes using an encoding that can't represent certain bytes, this raises encode/decode errors.
  - Scrapers may be reading bytes and decoding with an incorrect codec or writing to stdout/stderr using a non-UTF-8 encoding.
- Previous attempts:
  - ACM: runner.py now sets `PYTHONIOENCODING=utf-8` and `PYTHONUTF8=1` in the child process environment and spawns the child with `text=True, encoding='utf-8', errors='replace'`. This reduces many stdio encoding problems and provides live logs.
  - A proposed change (prepared by the assistant but not applied) was to add defensive decoding of scraped content with `errors='replace'` inside gpt-researcher scrapers.
- Recommended next steps:
  - Ensure scrapers detect encoding and decode with `errors='replace'` or chardet/charset-normalizer to correctly handle non-UTF8 content.
  - Keep `PYTHONIOENCODING` and `PYTHONUTF8` in the spawn env (already applied).
  - Optionally reduce scraper log verbosity for binary content to avoid repeated noisy errors.

### 2) GPTResearcher.write_report() returned no content (None / empty report)
- Symptom / evidence:
  - CSV run 3 (dr/gpt-5-mini) shows: "gpt-researcher failed: GPTResearcher.write_report() returned no content (programmatic run produced no generated files)"
  - For that run, runner attempted the subprocess path and could not parse a valid output path from child stdout.
- Root cause:
  - gpt-researcher programmatic pipeline produced no final report content (LLM returned empty, or internal validation rejected outputs and resulted in no write).
  - The client `run_gpt_researcher_programmatic` currently raises ValueError when `report_content` is falsy; in the current ACM flow, this bubbled up as an error and caused the run to be recorded as failed.
- Previous attempts:
  - The assistant prepared a patch to write a `.failed.json` artifact containing failure metadata (model, exception) instead of raising, but the user declined to apply edits at that time.
  - ACM's runner writes `.gptr.failed.json` / `.dr.failed.json` at the output folder level if no outputs were produced for a run group (this is a fallback artifact).
- Recommended next steps:
  - Apply a defensive change inside `api_cost_multiplier/functions/gpt_researcher_client.py` so that when `write_report()` returns falsy, the client writes a `.failed.json` file with the exception and context (model, provider, short log excerpt), then returns that path so runner can save it like other artifacts. This makes failures auditable and avoids upstream `NoneType` exceptions.
  - Add more verbose local logging in gpt-researcher when final content is empty (capture LLM API response payloads or error messages where possible).

### 3) Provider parameter validation: unsupported 'temperature'
- Symptom / evidence:
  - CSV run 4 (dr/gpt-5-nano) shows: "Unsupported value: 'temperature' does not support 0.4 with this model. Only the default (1) value is supported."
  - This is an invalid_request_error returned by the provider (OpenAI) when an unsupported parameter is included in the request.
- Root cause:
  - The pipeline (gpt-researcher or patched config) passed `temperature: 0.4` for a model that doesn't accept non-default temperatures. This indicates a mismatch between what config sets and model capabilities.
- Previous attempts:
  - No automated retry or parameter fallback was implemented; an assistant-prepared patch proposed retrying without temperature but was not applied.
  - The runner patches `gpt-researcher` default config by setting `SMART_LLM`/`STRATEGIC_LLM` to `provider:model`. It does not currently sanitize LLM-specific parameters per model.
- Recommended next steps:
  - Implement a provider-parameter hygiene layer:
    1. Option A (ACM): Before patching gpt-researcher config, consult a model capability map and avoid setting unsupported params (e.g., remove temperature for models that don't support it).
    2. Option B (gpt-researcher): Catch invalid_request_error indicating unsupported parameter names/values and retry the call after removing or resetting offending parameters (e.g., remove `temperature` or set to provider default).
  - Add a small test harness that probes model capability (send a minimal request) or uses a static capability mapping to avoid obvious invalid parameter combinations.

### 4) Silent / buffered subprocess output (apparent hang)
- Symptom / evidence:
  - Previously, parent used `subprocess.run(capture_output=True)` which buffered child stdout/stderr. User observed "no output" and perceived runs hanging.
- Root cause:
  - Child process performed long-running work; parent buffered stdout/stderr, so the operator saw no incremental logs.
- Previous attempts and fixes applied:
  - Runner was updated to spawn child with Popen and reader threads, and to set `PYTHONIOENCODING`/`PYTHONUTF8` in the child environment. This restored live logs with prefixes `[OUT]` and `[ERR]`.
- Recommendation:
  - Keep the streaming implementation and monitor memory/threads for long runs. Optionally, add per-subprocess timeouts and partial log dumps on timeout to aid diagnostics.

### 5) Third-party search failures (Tavily 400 errors)
- Symptom / evidence:
  - During runs, gpt-researcher logged 400 responses from Tavily (api.tavily.com/search) for some queries. These resulted in missing search results for research steps that depend on them.
- Root cause:
  - The external API returned Bad Request for certain query payloads. This is an external dependency failure.
- Previous attempts:
  - None automated; observed in runtime logs.
- Recommended next steps:
  - Add retries with exponential backoff for calls to third-party search services.
  - Sanitize or simplify queries before sending to Tavily.
  - Add a fallback to direct scraping if Tavily fails after N retries.

---

## Representative evidence (excerpts)

CSV lines (generate_run_results.csv):
- 0,gptr,openai,gpt-5, ... success ...
- 1,gptr,openai,gpt-5-mini, ... error: 'charmap' codec can't encode characters ...
- 3,dr,openai,gpt-5-mini, ... error: GPTResearcher.write_report() returned no content ...
- 4,dr,openai,gpt-5-nano, ... error: Unsupported value: 'temperature' does not support 0.4 ...

Sample successful temp report (first lines of a generated MD file):
- (temp_gpt_researcher_reports/research_report_38183dd4-7194-44d7-a732-ec78918ec36b.md)
  - "Latest update on the federal government shutdown (as of Oct. 4, 2025)"
  - Contains a structured summary and multiple sections — evidence that GPT-Researcher can produce full reports when LLM/provider and scrapers behave correctly.

Runner code evidence:
- `runner.py` sets `env.setdefault("PYTHONIOENCODING", "utf-8")` and `env.setdefault("PYTHONUTF8", "1")` before spawning gpt-researcher subprocesses.
- `runner.py` now uses `subprocess.Popen(..., stdout=PIPE, stderr=PIPE, text=True, encoding="utf-8", errors="replace")` + reader threads to stream logs.

gpt_researcher_client behavior:
- `run_gpt_researcher_programmatic` loads `.env`, instantiates GPTResearcher, calls `await researcher.conduct_research()`, then `await researcher.write_report()`.
- Current logic raises ValueError when `write_report()` returns falsy.

---

## Actionable remediation recommendations (priority ordered)

Short-term (low risk, high impact)
1. Add a defensive failure artifact: when programmatic `write_report()` returns no content, write a `.failed.json` containing metadata (model, provider, exception, short log excerpt) and return that path instead of raising. This prevents NoneType crashes upstream and provides an auditable artifact.
2. Catch provider invalid_request_error for unsupported parameters (e.g., temperature), then retry without the offending parameter once. Log this event explicitly.
3. Keep the runner's UTF-8 child env + streaming Popen approach (already implemented).

Medium-term
4. Update scrapers to decode bytes with `errors='replace'`, or detect and use the correct encoding, to eliminate noisy encoding exceptions.
5. Add retries/backoff and input sanitization for third-party search APIs (Tavily).

Long-term
6. Add per-model capability metadata (static mapping or probe) so ACM can avoid sending unsupported parameters for specific models, or so gpt-researcher can adapt its request parameters dynamically.
7. Add a small test harness that runs a quick probing run per model to detect incompatible parameter combinations before a full run, and report a clear error to the operator.

---

## Suggested next steps (for me to implement)

If you want this work applied now, I will:
1. Implement the failure-artifact change in `api_cost_multiplier/functions/gpt_researcher_client.py` and/or the runner logic to accept `.failed.json` artifacts.
2. Implement a provider-error handler to retry without `temperature` when provider raises an invalid_request_error citing unsupported parameters.
3. Optionally apply a small change to scrapers or suggest a PR to gpt-researcher to decode with `errors='replace'`.

I am ready to apply the code edits and re-run the pipeline. Toggle to Act mode (you already did) and confirm which of the above changes you want me to apply first.
