MA Issues Report — Focus: (1) Parent Unicode decoding, (4) LLM streaming permission, (5) Cancelled/KeyboardInterrupt cascades
========================================================================================
Date: 2025-08-26
Repository: c:/dev/hookie/process_markdown

Prepared for: Developer / Maintainer
Prepared by: Assistant (Cline)

Executive summary
-----------------
This document summarizes three high-priority issues observed during local runs of the process_markdown pipeline and the Multi-Agent (MA) CLI. The focus is on:
1) Parent-side Unicode decoding failures when streaming MA subprocess output into the parent process.
4) LLM API streaming permission errors (OpenAI streaming rejected by organization).
5) Cascading asyncio CancelledError / KeyboardInterrupts resulting from MA failures.

Each section below covers observed symptoms, root cause analysis, immediate mitigations applied, and recommended permanent fixes and next steps.

1) Parent Unicode decoding (symptoms & root cause)
-------------------------------------------------
Symptoms observed:
- Multiple UnicodeDecodeError exceptions thrown in threads reading subprocess stdout/stderr:
  - Example: UnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 24: character maps to <undefined>
- Exceptions occurred inside the MA runner thread `_reader` while calling stream.read(1) or decoding bytes read from the child.
- These exceptions caused thread crashes and contributed to MA run failures or noisy logs.

Root cause:
- Parent process opened subprocess pipes in text mode or relied on platform default decoding (cp1252 on Windows). The child process emitted bytes that are not representable in cp1252, causing decode errors in Python's text-mode reading.
- Setting PYTHONIOENCODING in child helps child use UTF-8, but parent still attempted to decode raw bytes using cp1252 in some circumstances. That mismatch caused errors.

Mitigations already applied (by assistant):
- MA subprocess environment now explicitly sets PYTHONIOENCODING="utf-8".
- MA runner's stdout/stderr reader was reworked to forward output character-by-character and to prefix each logical line with the MA run id.
- This reduced some Unicode problems but did not fully eliminate parent decode errors in initial runs.

Recommended permanent fix:
- Parent must decode safely; apply one of these options in process_markdown/functions/MA_runner.py before creating Popen:
  Option A (recommended, minimal):
    - Use subprocess.Popen with explicit encoding and error handling:
      subprocess.Popen(..., stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                       text=True, encoding="utf-8", errors="replace", ...)
    - This ensures the parent decodes using UTF-8 and replaces invalid bytes rather than raising.
  Option B (more robust):
    - Open pipes in binary and decode bytes manually in the reader with errors="replace":
      raw = stream.buffer.read(1)
      ch = raw.decode("utf-8", errors="replace")
    - This gives full control and avoids platform text-decoding surprises.

Why recommended:
- Ensures no UnicodeDecodeError in parent even if child emits invalid bytes.
- Keeps live output (character-level forwarding) while preventing thread crashes.

Files/locations:
- process_markdown/functions/MA_runner.py — _reader implementation and Popen call.

2) LLM API streaming permission (symptoms & root cause)
-------------------------------------------------------
Symptoms observed:
- HTTP 400 errors returned by model provider when code attempted to use stream=True:
  - "Your organization must be verified to stream this model. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. ... code: 'unsupported_value'"
- These occurred when gpt_researcher attempted streaming with GPT-5 family models.

Root cause:
- OpenAI (or provider) account lacks streaming permission for certain models (GPT-5 family). Attempting to pass stream=True produced a provider-side 400 error.
- Prior code attempted streaming by default in some programmatic flows.

Mitigations already applied (by assistant):
- Enforced a global env toggle GPTR_DISABLE_STREAMING default true in MA runner subprocess env and respected in the create_chat_completion call:
  - process_markdown/functions/MA_runner.py sets env.setdefault("GPTR_DISABLE_STREAMING", "true")
  - process_markdown/gpt-researcher/gpt_researcher/utils/llm.py reads GPTR_DISABLE_STREAMING and sets effective_stream = stream and (not disable_streaming)
- Implemented retry logic: if a streaming permission error is returned, utils/llm.py detects the message and retries the request with stream=False.
- Listed models that do not accept temperature in NO_SUPPORT_TEMPERATURE_MODELS (including GPT-5 variants) to avoid temperature-related 400s.

Recommended permanent fix / hardening:
- Keep GPTR_DISABLE_STREAMING=true as default for programmatic runs.
- Add a clear env var documentation and optional CLI flags to override behavior per environment (e.g., GPTR_DISABLE_STREAMING=true/false).
- Ensure create_chat_completion logs (redacting keys) the effective_stream flag and canonicalized model name for easier debugging.
- If you want streaming in some environments, implement streaming→non-stream fallback (already partially implemented) and add an explicit env toggle for trusted environments.

Files/locations:
- process_markdown/gpt-researcher/gpt_researcher/utils/llm.py — canonicalization, disable_streaming logic, retries without temperature.
- process_markdown/functions/MA_runner.py — sets GPTR_DISABLE_STREAMING for MA subprocess env.

3) CancelledError / KeyboardInterrupt cascades
----------------------------------------------
Symptoms observed:
- asyncio.exceptions.CancelledError and KeyboardInterrupt raised during multi-agent orchestration (langgraph runner stack).
- These cascaded from MA tasks being cancelled or thrown due to underlying errors (UnicodeDecodeError, other exceptions) and then bubbled up causing the parent to cancel other tasks; sometimes the parent generated KeyboardInterrupt on run termination.

Root cause:
- Unhandled exceptions inside individual MA tasks caused langgraph/asyncio to cancel related tasks.
- Parent process sometimes held gather(...) of gpt_programmatic tasks; cancelled children lead to CancelledError propagation.

Mitigations applied (by assistant):
- The MA runner now collects per-run stdout/stderr and prints errors; when a run fails the runner prints "MA run N failed: ..." and continues other runs (try/except around run_multi_agent_once).
- Improvements to Unicode handling and env fixes reduced the frequency of fatal exceptions that caused large cancellations.
- Added robust character-forwarding and per-line prefix to make error lines easier to spot.

Recommended permanent fixes:
- Enhance MA CLI and multi-agent code to catch exceptions within agent steps and return structured error objects instead of raising. That prevents whole run cancellation.
- Within generate.py and run orchestration:
  - Use asyncio.shield or structured task cancellation strategies for better isolation.
  - Always catch CancelledError where appropriate and log the root exception.
- Ensure per-run logs are written to disk for postmortem so cancelled runs can be analyzed offline.

Files/locations:
- process_markdown/MA_CLI/Multi_Agent_CLI.py — entrypoint where chief_editor.run_research_task(...) is invoked (tracebacks pointed here).
- process_markdown/gpt-researcher/multi_agents/* (orchestrator, writer, editor) — where exceptions arose.
- process_markdown/functions/MA_runner.py — wrapper that runs MA CLI subprocesses and now handles per-run exceptions.

Collected errors and warnings from the last run (verbatim excerpts)
-------------------------------------------------------------------
- Unicode decode exceptions during _reader in MA_runner:
  - "UnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 24: character maps to <undefined>"
  - (Occurred in multiple reader threads for different MA runs)
- LLM streaming permission:
  - "Error code: 400 - {'error': {'message': 'Your organization must be verified to stream this model...','code':'unsupported_value'}}"
  - Observed earlier before enforcing GPTR_DISABLE_STREAMING.
- WeasyPrint native lib error (PDF conversion failure):
  - "Error in converting Markdown to PDF: cannot load library 'libgobject-2.0-0': error 0x7e."
- SSL certificate verification failures while scraping PDFs:
  - "SSLError: certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)"
- asyncio cancellation and KeyboardInterrupt:
  - "asyncio.exceptions.CancelledError"
  - Followed by "KeyboardInterrupt" in the top-level runner.

Actions already taken (chronological)
------------------------------------
1. Cloned repository, ran download_and_extract.py to fetch gpt-researcher and llm-doc-eval modules.
2. Created generate_gptr_only.py for GPT-R only runs.
3. Merged process_markdown/config.yaml with gpt_researcher DEFAULT_CONFIG so missing keys are filled.
4. Implemented llm.py changes: canonicalize model names, add GPTR_DISABLE_STREAMING check, add retry logic for streaming/temperature errors.
5. Implemented MA_runner changes:
   - Set GPTR_DISABLE_STREAMING=true
   - Set PYTHONIOENCODING=utf-8
   - Replaced _reader to provide live output plus per-line "[MA run N]" prefix.
6. Re-ran generate.py and observed output: MA runs printed per-line with prefixes; GPT-R runs executed; errors above recorded.
7. Recommended parent decode fix; not yet applied (pending user confirmation).

Recommended next steps (actionable, immediate)
----------------------------------------------
A. (High priority) Fix parent decoding to fully eliminate UnicodeDecodeError:
   - Apply one of these in process_markdown/functions/MA_runner.py:
     - Set Popen(..., text=True, encoding='utf-8', errors='replace') — simplest and recommended.
     - OR read bytes from stream and decode with errors='replace' inside _reader.
   - Re-run generate.py; verify no UnicodeDecodeError thrown in reader threads.

B. Verify LLM streaming is indeed disabled:
   - Confirm utils/llm.py reports disable_streaming in logs for LLM calls.
   - Keep GPTR_DISABLE_STREAMING=true in MA runner env.

C. Harden cancellation handling:
   - Add try/except around langgraph chain invocations in MA CLI to convert exceptions to logged error results rather than letting them propagate as CancelledErrors when possible.
   - Ensure MA_runner prints and stores run-specific stdout/stderr logs to a file for per-run postmortem.

D. Environment fixes (non-code):
   - pip install --upgrade certifi OR set REQUESTS_CA_BUNDLE pointing to certifi.where() to fix SSL errors.
   - Install WeasyPrint native dependencies for PDF conversion on Windows or skip conversion.

If you want me to implement (A) now, I will:
- Modify process_markdown/functions/MA_runner.py to pass encoding='utf-8' and errors='replace' to subprocess.Popen (or implement bytes-decode in the reader).
- Re-run generate.py and report the outcomes (confirm no Unicode exceptions, MA run prefixes still present, and that LLM streaming was not attempted).

Please reply with one of:
- "apply parent decode fix and run" — I will edit MA_runner.py and re-run generate.py (Act Mode).
- "show patch" — I will paste the exact code diff for review before applying.
- Or, if you prefer to handle environment steps first (certifi/WeasyPrint), say which to prioritize.

</response>
</plan_mode_respond>
