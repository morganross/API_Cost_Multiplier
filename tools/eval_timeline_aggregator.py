#!/usr/bin/env python3
"""
eval_timeline_aggregator.py

Aggregates evaluation timeline data from multiple sources:
- Config files (expected runs)
- FPF logs (actual execution timing, costs, tokens)
- SQLite database (evaluation results)
- CSV exports (fallback for archived runs)

This module generates a unified view correlating Expected runs (what should happen
based on config) with Actual execution data (what really happened).

Usage:
    from tools.eval_timeline_aggregator import EvalTimelineAggregator
    
    aggregator = EvalTimelineAggregator(
        config_path="config.yaml",
        eval_config_path="llm-doc-eval/config.yaml",
        db_path="results.sqlite",
        fpf_logs_dir="FilePromptForge/logs/run_group_id"
    )
    chart = aggregator.generate_chart()  # Returns TimelineChart dataclass
    chart_dict = aggregator.to_dict()     # Returns JSON-serializable dict

Architecture Notes:
- Pass dicts (not class instances) to html_exporter to avoid circular dependencies
- Logs are the authoritative source for cost/token data; DB/CSV never synthesize costs
- Deduplication by (run_group_id, run_id, file_path)
- Matching uses tiered keys: exact -> ordinal -> partial
"""

from __future__ import annotations

import hashlib
import json
import logging
import os
import re
import sqlite3
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum
from itertools import combinations
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import yaml


# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
logger = logging.getLogger(__name__)


# -----------------------------------------------------------------------------
# Enums
# -----------------------------------------------------------------------------
class EvalPhase(Enum):
    """
    Evaluation phases in execution order.
    
    Phase 1: Pre-combine single-doc evaluation (each doc judged individually)
    Phase 2: Pre-combine pairwise evaluation (head-to-head comparisons)
    Phase 3: Combiner generation (merge top documents)
    Phase 4: Post-combine single-doc evaluation (including combined doc)
    Phase 5: Post-combine pairwise evaluation (final rankings)
    """
    PRECOMBINE_SINGLE = "precombine-single-eval"
    PRECOMBINE_PAIRWISE = "precombine-pairwise-eval"
    COMBINER = "combiner-generation"
    POSTCOMBINE_SINGLE = "postcombine-single-eval"
    POSTCOMBINE_PAIRWISE = "postcombine-pairwise-eval"

    @property
    def display_name(self) -> str:
        """Human-readable phase name for display."""
        return {
            EvalPhase.PRECOMBINE_SINGLE: "Phase 1: Pre-Combine Single Eval",
            EvalPhase.PRECOMBINE_PAIRWISE: "Phase 2: Pre-Combine Pairwise Eval",
            EvalPhase.COMBINER: "Phase 3: Combiner Generation",
            EvalPhase.POSTCOMBINE_SINGLE: "Phase 4: Post-Combine Single Eval",
            EvalPhase.POSTCOMBINE_PAIRWISE: "Phase 5: Post-Combine Pairwise Eval",
        }.get(self, self.value)

    @property
    def short_name(self) -> str:
        """Short phase identifier for compact display."""
        return {
            EvalPhase.PRECOMBINE_SINGLE: "pre-s",
            EvalPhase.PRECOMBINE_PAIRWISE: "pre-p",
            EvalPhase.COMBINER: "comb",
            EvalPhase.POSTCOMBINE_SINGLE: "post-s",
            EvalPhase.POSTCOMBINE_PAIRWISE: "post-p",
        }.get(self, self.value[:6])


class SourceType(Enum):
    """Data source types for actual run data."""
    LOGS = "logs"
    SQLITE = "sqlite"
    CSV = "csv"
    CONFIG_ONLY = "config_only"  # Expected run with no actual data found


class MatchStatus(Enum):
    """Status of expected-to-actual matching."""
    EXACT = "exact"        # Matched by (phase, judge, target)
    ORDINAL = "ordinal"    # Matched by (phase, judge, ordinal_index)
    PARTIAL = "partial"    # Matched by phase only
    MISSING = "missing"    # No actual data found


# -----------------------------------------------------------------------------
# Dataclasses: Source Data
# -----------------------------------------------------------------------------
@dataclass
class ExpectedRun:
    """
    Expected run from config files (SPEC 1).
    
    Represents what *should* execute based on the configuration.
    Generated by cross-product of judges × targets for each phase.
    """
    run_num: int                   # Sequential run number (1-indexed)
    phase: EvalPhase               # Evaluation phase
    judge_model: str               # Judge model identifier (e.g., "google_gemini-2.5-flash")
    target: str                    # Document or pair being evaluated
    
    # Auditability fields (per 3rd/4th party review)
    expected_id: str = ""          # Stable hash of (phase, judge, target) for matching
    expected_index: int = 0        # Ordinal within phase+judge for fallback matching
    
    def __post_init__(self):
        """Generate expected_id if not provided."""
        if not self.expected_id:
            key = f"{self.phase.value}:{self.judge_model}:{self.target}"
            self.expected_id = hashlib.md5(key.encode()).hexdigest()[:12]


@dataclass
class ActualRunLog:
    """
    Actual run data from FPF JSON logs (SPEC 2).
    
    Contains timing, cost, and token data from executed LLM calls.
    This is the authoritative source for cost/token data.
    """
    run_id: str                              # Unique run identifier
    run_group_id: Optional[str]              # Group identifier for related runs
    model: str                               # Model used (e.g., "gemini-2.5-flash")
    provider: str                            # Provider (e.g., "google", "openai")
    started_at: datetime                     # Execution start time
    finished_at: datetime                    # Execution end time
    duration_seconds: float                  # Execution duration
    
    # Token/cost data (from logs only)
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    total_cost_usd: float = 0.0
    
    # Source tracking
    log_file_path: str = ""                  # Path to source log file
    is_failure: bool = False                 # True if from failure-*.json
    
    # Deduplication key
    dedup_key: str = ""                      # (run_group_id, run_id, file_path)
    
    # For matching to expected
    detected_phase: Optional[EvalPhase] = None
    detected_target: Optional[str] = None
    
    def __post_init__(self):
        """Generate dedup_key."""
        if not self.dedup_key:
            self.dedup_key = f"{self.run_group_id or ''}:{self.run_id}:{self.log_file_path}"


@dataclass
class DbRunResult:
    """
    Run results from SQLite database (SPEC 3).
    
    Contains result counts and timestamps, but NOT costs/tokens.
    """
    doc_id: str                              # Document identifier
    model: str                               # Judge model
    eval_type: str                           # "single" or "pairwise"
    row_count: int                           # Number of result rows
    first_timestamp: Optional[datetime]       # MIN(timestamp) for this judge+doc
    last_timestamp: Optional[datetime]        # MAX(timestamp) for this judge+doc
    duration_seconds: float = 0.0            # MAX - MIN
    
    # For pairwise
    doc_id_2: Optional[str] = None           # Second document in pair


@dataclass
class CsvRunData:
    """
    Run data from CSV exports (SPEC 4 - fallback).
    
    Used when database is unavailable or for archived runs.
    """
    doc_id: str
    model: str
    eval_type: str
    row_count: int
    timestamp: Optional[str] = None          # May have timestamp column
    source_file: str = ""


# -----------------------------------------------------------------------------
# Dataclasses: Output Schema
# -----------------------------------------------------------------------------
@dataclass
class TimelineRow:
    """
    Complete timeline row combining all sources.
    
    This is the primary output row shown in the HTML chart.
    """
    # --- Expected (from config) ---
    run_num: int                             # Sequential run number
    phase: str                               # Phase value (enum string)
    phase_display: str                       # Human-readable phase name
    phase_short: str                         # Short phase identifier
    judge_model: str                         # Judge model identifier
    target: str                              # Full target (doc_id or pair)
    target_short: str                        # Truncated target for display
    
    # --- Actual (from logs) ---
    log_start: Optional[str] = None          # ISO timestamp
    log_end: Optional[str] = None            # ISO timestamp
    log_duration_seconds: Optional[float] = None
    log_duration_display: str = "—"          # "mm:ss" or "—"
    
    # --- DB Results ---
    db_row_count: int = 0
    db_first_ts: Optional[str] = None
    db_last_ts: Optional[str] = None
    db_duration_seconds: Optional[float] = None
    db_duration_display: str = "—"
    
    # --- Costs/Tokens (from logs only) ---
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    total_cost_usd: float = 0.0
    cost_display: str = "—"                  # "$0.0123" or "—"
    
    # --- Status/Matching ---
    matched: bool = False                    # Expected run found in actual
    match_status: str = "missing"            # MatchStatus value
    delta_seconds: Optional[float] = None    # Log duration - DB duration
    delta_display: str = "—"
    status_icon: str = "✗"                   # ✓, ⚠, ✗
    
    # --- Auditability (per 3rd/4th party review) ---
    expected_id: str = ""                    # Stable hash for matching
    expected_index: int = 0                  # Ordinal for fallback matching
    source_used: str = "config_only"         # SourceType value
    
    # --- Retry handling (per 4th party review) ---
    retry_count: int = 0                     # 0 = no retries
    is_retry: bool = False                   # True if this row is a retry attempt


@dataclass
class PhaseSubtotal:
    """Subtotal aggregation for a phase."""
    phase: str
    phase_display: str
    run_count: int                           # Number of expected runs
    matched_count: int                       # Number successfully matched
    expected_count: int                      # Same as run_count (for clarity)
    
    # Duration totals
    total_duration_seconds: float = 0.0
    total_duration_display: str = "00:00"
    
    # Token/cost totals
    total_prompt_tokens: int = 0
    total_completion_tokens: int = 0
    total_tokens: int = 0
    total_cost_usd: float = 0.0
    cost_display: str = "$0.0000"
    
    # DB row totals
    total_db_rows: int = 0


@dataclass
class TimelineChart:
    """
    Complete timeline chart data.
    
    This is the top-level output passed to the HTML exporter.
    Must be converted to dict via asdict() or to_dict() before passing
    to html_exporter to avoid circular dependencies.
    """
    # --- Main data ---
    rows: List[TimelineRow] = field(default_factory=list)
    phase_subtotals: List[PhaseSubtotal] = field(default_factory=list)
    grand_total: Optional[PhaseSubtotal] = None
    
    # --- Unmatched actuals (per 4th party review) ---
    unplanned_actuals: List[Dict[str, Any]] = field(default_factory=list)
    
    # --- Metadata ---
    run_start: Optional[str] = None          # Earliest timestamp across all
    run_end: Optional[str] = None            # Latest timestamp across all
    total_run_count: int = 0
    total_matched_count: int = 0
    
    # --- Config snapshot (per 4th party review) ---
    config_snapshot: Dict[str, Any] = field(default_factory=dict)
    
    # --- Source paths ---
    sources: Dict[str, str] = field(default_factory=dict)
    
    # --- Generation metadata ---
    generated_at: str = ""
    generator_version: str = "1.0.0"


# -----------------------------------------------------------------------------
# Helper Functions
# -----------------------------------------------------------------------------
def parse_iso_ts(ts_str: Optional[str]) -> Optional[datetime]:
    """
    Parse ISO format timestamp with various formats.
    
    Handles:
    - "2025-11-28T12:00:00.123456Z"
    - "2025-11-28T12:00:00Z"
    - "2025-11-28 12:00:00"
    - "2025-11-28T12:00:00+00:00"
    """
    if not ts_str:
        return None
    try:
        # Strip trailing Z and timezone
        ts_str = ts_str.rstrip("Z")
        if "+" in ts_str:
            ts_str = ts_str.split("+")[0]
        if ts_str.endswith("-00:00"):
            ts_str = ts_str[:-6]
        
        # Handle space separator
        ts_str = ts_str.replace(" ", "T")
        
        return datetime.fromisoformat(ts_str)
    except Exception:
        return None


def format_duration(seconds: Optional[float]) -> str:
    """Convert seconds to mm:ss or hh:mm:ss format."""
    if seconds is None:
        return "—"
    total_seconds = int(round(seconds))
    if total_seconds < 0:
        return "—"
    
    hours, remainder = divmod(total_seconds, 3600)
    minutes, secs = divmod(remainder, 60)
    
    if hours > 0:
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    return f"{minutes:02d}:{secs:02d}"


def format_cost(cost: Optional[float]) -> str:
    """Format cost as $X.XXXX or '—' if missing/zero."""
    if cost is None or cost == 0.0:
        return "—"
    return f"${cost:.4f}"


def truncate_target(target: str, max_len: int = 30) -> str:
    """Truncate long target strings for display."""
    if len(target) <= max_len:
        return target
    return target[:max_len - 3] + "..."


def validate_fpf_logs_path(fpf_logs_dir: str, base_dir: str) -> bool:
    """
    Validate that FPF logs directory is within expected path.
    
    Path guard per 3rd/4th party review - ensures logs are under
    api_cost_multiplier/FilePromptForge/logs to prevent path confusion.
    """
    if not fpf_logs_dir:
        return False
    
    try:
        fpf_path = Path(fpf_logs_dir).resolve()
        # Check if it's under FilePromptForge/logs or a valid copy location
        valid_patterns = [
            "FilePromptForge/logs",
            "FilePromptForge\\logs",
            "logs/eval_fpf_logs",
            "logs\\eval_fpf_logs",
        ]
        fpf_str = str(fpf_path).replace("\\", "/").lower()
        return any(pattern.replace("\\", "/").lower() in fpf_str for pattern in valid_patterns)
    except Exception:
        return False


# -----------------------------------------------------------------------------
# Main Aggregator Class
# -----------------------------------------------------------------------------
class EvalTimelineAggregator:
    """
    Main aggregator class combining all data sources.
    
    Generates a unified timeline chart correlating expected runs (from config)
    with actual execution data (from logs, database, CSV).
    
    Architecture notes:
    - Pass dicts to html_exporter (not class instances) to avoid circular deps
    - Logs are authoritative for cost/tokens
    - Dedup by (run_group_id, run_id, file_path)
    - Matching uses tiered keys: exact -> ordinal -> partial
    """
    
    def __init__(
        self,
        config_path: str,
        eval_config_path: str,
        db_path: Optional[str] = None,
        fpf_logs_dir: Optional[str] = None,
        csv_export_dir: Optional[str] = None,
        acm_log_path: Optional[str] = None,
        time_window_start: Optional[str] = None,
        time_window_end: Optional[str] = None,
    ):
        """
        Initialize the aggregator.
        
        Args:
            config_path: Path to main config.yaml (runs[], combine, eval settings)
            eval_config_path: Path to llm-doc-eval/config.yaml (judge models)
            db_path: Path to SQLite results database (optional)
            fpf_logs_dir: Path to FPF JSON logs directory (optional)
            csv_export_dir: Path to CSV exports directory (fallback, optional)
            acm_log_path: Path to ACM session log (optional, for additional signals)
            time_window_start: ISO timestamp to filter logs (optional)
            time_window_end: ISO timestamp to filter logs (optional)
        """
        self.config_path = config_path
        self.eval_config_path = eval_config_path
        self.db_path = db_path
        self.fpf_logs_dir = fpf_logs_dir
        self.csv_export_dir = csv_export_dir
        self.acm_log_path = acm_log_path
        self.time_window_start = time_window_start
        self.time_window_end = time_window_end
        
        # Cached config data
        self._config: Optional[Dict[str, Any]] = None
        self._eval_config: Optional[Dict[str, Any]] = None
        
        # Path validation (per 3rd/4th party review)
        if fpf_logs_dir:
            base_dir = os.path.dirname(config_path) if config_path else ""
            if not validate_fpf_logs_path(fpf_logs_dir, base_dir):
                logger.warning(
                    f"FPF logs directory may be outside expected path: {fpf_logs_dir}. "
                    "Expected under FilePromptForge/logs or logs/eval_fpf_logs."
                )
    
    # -------------------------------------------------------------------------
    # Config Loading
    # -------------------------------------------------------------------------
    def _load_config(self) -> Dict[str, Any]:
        """Load and cache main config.yaml."""
        if self._config is None:
            if self.config_path and os.path.isfile(self.config_path):
                with open(self.config_path, "r", encoding="utf-8") as f:
                    self._config = yaml.safe_load(f) or {}
            else:
                logger.warning(f"Config file not found: {self.config_path}")
                self._config = {}
        return self._config
    
    def _load_eval_config(self) -> Dict[str, Any]:
        """Load and cache llm-doc-eval/config.yaml."""
        if self._eval_config is None:
            if self.eval_config_path and os.path.isfile(self.eval_config_path):
                with open(self.eval_config_path, "r", encoding="utf-8") as f:
                    self._eval_config = yaml.safe_load(f) or {}
            else:
                logger.warning(f"Eval config file not found: {self.eval_config_path}")
                self._eval_config = {}
        return self._eval_config
    
    def _get_eval_mode(self) -> str:
        """Get evaluation mode from config (single|pairwise|both)."""
        config = self._load_config()
        return config.get("eval", {}).get("mode", "both")
    
    def _get_pairwise_top_n(self) -> int:
        """Get pairwise_top_n from config."""
        config = self._load_config()
        return config.get("eval", {}).get("pairwise_top_n", 3)
    
    def _is_combine_enabled(self) -> bool:
        """Check if combine phase is enabled."""
        config = self._load_config()
        return config.get("combine", {}).get("enabled", False)
    
    def _get_combiner_limit(self) -> int:
        """
        Get combiner limit (hardcoded to 2 per Spec 1).
        
        TODO: Make configurable via config.yaml per 4th party review.
        """
        return 2  # Hardcoded per Spec 1 notes
    
    def _get_judge_models(self) -> List[str]:
        """Get list of judge model identifiers from eval config."""
        eval_config = self._load_eval_config()
        models = eval_config.get("models", {})
        return list(models.keys())
    
    def _get_combiner_models(self) -> List[Tuple[str, str]]:
        """Get list of combiner models as (provider, model) tuples."""
        config = self._load_config()
        combine_models = config.get("combine", {}).get("models", [])
        result = []
        for cm in combine_models:
            provider = cm.get("provider", "")
            model = cm.get("model", "")
            result.append((provider, model))
        return result
    
    def _get_generated_docs(self) -> List[str]:
        """
        Get list of generated document identifiers from config runs.
        
        Returns list like ["fpf-google-gemini-2.5-flash", "gptr-openai-gpt-5-mini", ...]
        """
        config = self._load_config()
        runs = config.get("runs", [])
        doc_ids = []
        for run in runs:
            run_type = run.get("type", "")
            provider = run.get("provider", "")
            model = run.get("model", "")
            doc_id = f"{run_type}-{provider}-{model}"
            doc_ids.append(doc_id)
        return doc_ids
    
    # -------------------------------------------------------------------------
    # Expected Runs Generation (SPEC 1)
    # -------------------------------------------------------------------------
    def generate_expected_runs(self) -> List[ExpectedRun]:
        """
        Generate expected runs from config files (SPEC 1).
        
        Honors eval.mode (single|pairwise|both) for phase gating.
        Only emits combiner rows when combine.enabled is true.
        
        Returns:
            List of ExpectedRun objects in execution order.
        """
        expected: List[ExpectedRun] = []
        run_num = 0
        
        mode = self._get_eval_mode()
        pairwise_top_n = self._get_pairwise_top_n()
        combine_enabled = self._is_combine_enabled()
        combiner_limit = self._get_combiner_limit()
        
        judges = self._get_judge_models()
        doc_ids = self._get_generated_docs()
        combiner_models = self._get_combiner_models()
        
        # Track ordinal index within each (phase, judge) for matching
        phase_judge_ordinals: Dict[str, int] = {}
        
        def get_ordinal(phase: EvalPhase, judge: str) -> int:
            key = f"{phase.value}:{judge}"
            phase_judge_ordinals[key] = phase_judge_ordinals.get(key, 0) + 1
            return phase_judge_ordinals[key]
        
        # --- Phase 1: Pre-combine Single Eval ---
        if mode in ("single", "both"):
            for judge in judges:
                for doc_id in doc_ids:
                    run_num += 1
                    expected.append(ExpectedRun(
                        run_num=run_num,
                        phase=EvalPhase.PRECOMBINE_SINGLE,
                        judge_model=judge,
                        target=doc_id,
                        expected_index=get_ordinal(EvalPhase.PRECOMBINE_SINGLE, judge),
                    ))
        
        # --- Phase 2: Pre-combine Pairwise Eval ---
        if mode in ("pairwise", "both"):
            for judge in judges:
                # Generate pairs from top-N
                for (i, j) in combinations(range(1, pairwise_top_n + 1), 2):
                    run_num += 1
                    target = f"Top#{i} vs Top#{j}"
                    expected.append(ExpectedRun(
                        run_num=run_num,
                        phase=EvalPhase.PRECOMBINE_PAIRWISE,
                        judge_model=judge,
                        target=target,
                        expected_index=get_ordinal(EvalPhase.PRECOMBINE_PAIRWISE, judge),
                    ))
        
        # --- Phase 3: Combiner Generation ---
        if combine_enabled:
            for provider, model in combiner_models:
                run_num += 1
                target = f"Combine Top {combiner_limit} Reports"
                judge = f"{provider}_{model}"
                expected.append(ExpectedRun(
                    run_num=run_num,
                    phase=EvalPhase.COMBINER,
                    judge_model=judge,
                    target=target,
                    expected_index=get_ordinal(EvalPhase.COMBINER, judge),
                ))
        
        # --- Phase 4: Post-combine Single Eval ---
        if combine_enabled and mode in ("single", "both"):
            # Pool includes: top-2 from pre-combine + combined docs
            post_pool_size = combiner_limit + len(combiner_models)
            post_targets = [f"Top#{i}" for i in range(1, combiner_limit + 1)]
            post_targets += [f"Combined-{p}-{m}" for p, m in combiner_models]
            
            for judge in judges:
                for target in post_targets:
                    run_num += 1
                    expected.append(ExpectedRun(
                        run_num=run_num,
                        phase=EvalPhase.POSTCOMBINE_SINGLE,
                        judge_model=judge,
                        target=target,
                        expected_index=get_ordinal(EvalPhase.POSTCOMBINE_SINGLE, judge),
                    ))
        
        # --- Phase 5: Post-combine Pairwise Eval ---
        if combine_enabled and mode in ("pairwise", "both"):
            post_pool_size = combiner_limit + len(combiner_models)
            effective_top_n = min(pairwise_top_n, post_pool_size)
            
            for judge in judges:
                for (i, j) in combinations(range(1, effective_top_n + 1), 2):
                    run_num += 1
                    target = f"Post Top#{i} vs Top#{j}"
                    expected.append(ExpectedRun(
                        run_num=run_num,
                        phase=EvalPhase.POSTCOMBINE_PAIRWISE,
                        judge_model=judge,
                        target=target,
                        expected_index=get_ordinal(EvalPhase.POSTCOMBINE_PAIRWISE, judge),
                    ))
        
        logger.info(f"Generated {len(expected)} expected runs")
        return expected
    
    # -------------------------------------------------------------------------
    # FPF Logs Parsing (SPEC 2)
    # -------------------------------------------------------------------------
    def parse_fpf_logs(self) -> Dict[str, ActualRunLog]:
        """
        Parse FPF logs for actual execution data (SPEC 2).
        
        Handles:
        - Regular success logs (*.json)
        - Failure logs (failure-*.json) marked with is_failure=True
        - Deduplication by (run_group_id, run_id, file_path)
        - Time window filtering if specified
        
        Returns:
            Dict mapping dedup_key -> ActualRunLog
        """
        logs: Dict[str, ActualRunLog] = {}
        
        if not self.fpf_logs_dir or not os.path.isdir(self.fpf_logs_dir):
            logger.warning(f"FPF logs directory not found: {self.fpf_logs_dir}")
            return logs
        
        # Parse time window
        tw_start = parse_iso_ts(self.time_window_start)
        tw_end = parse_iso_ts(self.time_window_end)
        
        def process_log_file(log_path: str, is_failure: bool = False):
            """Process a single FPF JSON log file."""
            try:
                with open(log_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                
                # Extract timestamps
                started_at_str = data.get("started_at", "")
                finished_at_str = data.get("finished_at", "")
                started_at = parse_iso_ts(started_at_str)
                finished_at = parse_iso_ts(finished_at_str)
                
                # Time window filter
                if tw_start and started_at and started_at < tw_start:
                    return
                if tw_end and started_at and started_at > tw_end:
                    return
                
                # Calculate duration
                duration = 0.0
                if started_at and finished_at:
                    duration = (finished_at - started_at).total_seconds()
                
                # Extract usage/cost
                usage = data.get("usage", {})
                cost = data.get("cost", {})
                
                run_log = ActualRunLog(
                    run_id=data.get("run_id", "unknown"),
                    run_group_id=data.get("run_group_id"),
                    model=data.get("model", "unknown"),
                    provider=data.get("config", {}).get("provider", "unknown"),
                    started_at=started_at or datetime.min,
                    finished_at=finished_at or datetime.min,
                    duration_seconds=duration,
                    prompt_tokens=usage.get("prompt_tokens", 0) or 0,
                    completion_tokens=usage.get("completion_tokens", 0) or 0,
                    total_tokens=usage.get("total_tokens", 0) or 0,
                    total_cost_usd=data.get("total_cost_usd", 0.0) or cost.get("total_cost_usd", 0.0) or 0.0,
                    log_file_path=log_path,
                    is_failure=is_failure,
                )
                
                # Detect phase and target from run_id pattern
                run_log.detected_phase = self._detect_phase_from_run_id(run_log.run_id)
                run_log.detected_target = self._extract_target_from_run_id(run_log.run_id)
                
                # Dedup: keep earliest start if duplicate
                if run_log.dedup_key in logs:
                    existing = logs[run_log.dedup_key]
                    if run_log.started_at < existing.started_at:
                        logs[run_log.dedup_key] = run_log
                else:
                    logs[run_log.dedup_key] = run_log
                    
            except Exception as e:
                logger.warning(f"Failed to parse FPF log {log_path}: {e}")
        
        # Walk directory (handle flat and nested structures)
        for item in os.listdir(self.fpf_logs_dir):
            item_path = os.path.join(self.fpf_logs_dir, item)
            
            if os.path.isfile(item_path) and item.endswith(".json"):
                is_failure = item.startswith("failure-")
                process_log_file(item_path, is_failure)
            elif os.path.isdir(item_path) and item != "validation":
                # Subdirectory (run_group_id folder)
                for subitem in os.listdir(item_path):
                    if subitem.endswith(".json"):
                        is_failure = subitem.startswith("failure-")
                        process_log_file(os.path.join(item_path, subitem), is_failure)
        
        logger.info(f"Parsed {len(logs)} FPF log entries")
        return logs
    
    def _detect_phase_from_run_id(self, run_id: str) -> Optional[EvalPhase]:
        """Detect evaluation phase from run_id pattern."""
        run_id_lower = run_id.lower()
        
        if "pairwise" in run_id_lower and "post" in run_id_lower:
            return EvalPhase.POSTCOMBINE_PAIRWISE
        elif "pairwise" in run_id_lower:
            return EvalPhase.PRECOMBINE_PAIRWISE
        elif "single" in run_id_lower and "post" in run_id_lower:
            return EvalPhase.POSTCOMBINE_SINGLE
        elif "single" in run_id_lower:
            return EvalPhase.PRECOMBINE_SINGLE
        elif "combin" in run_id_lower:
            return EvalPhase.COMBINER
        
        return None
    
    def _extract_target_from_run_id(self, run_id: str) -> Optional[str]:
        """Extract target (doc_id or pair) from run_id pattern."""
        # Pattern: single-{provider}-{model}-{doc_id}-...
        # Pattern: pairwise-{provider}-{model}-{doc1}-vs-{doc2}-...
        parts = run_id.split("-")
        if len(parts) >= 4:
            if "vs" in parts:
                # Pairwise: find the vs separator
                try:
                    vs_idx = parts.index("vs")
                    if vs_idx >= 4:
                        doc1 = "-".join(parts[3:vs_idx])
                        doc2 = "-".join(parts[vs_idx+1:vs_idx+4]) if len(parts) > vs_idx + 3 else parts[vs_idx+1]
                        return f"{doc1} vs {doc2}"
                except (ValueError, IndexError):
                    pass
            # Single: doc_id is after provider-model
            return parts[3] if len(parts) > 3 else None
        return None
    
    # -------------------------------------------------------------------------
    # Database Queries (SPEC 3)
    # -------------------------------------------------------------------------
    def query_db_results(self) -> Dict[str, DbRunResult]:
        """
        Query database for result data (SPEC 3).
        
        Returns:
            Dict mapping key -> DbRunResult
            Key format: "{eval_type}:{model}:{doc_id}" or "{eval_type}:{model}:{doc1}:{doc2}"
        """
        results: Dict[str, DbRunResult] = {}
        
        if not self.db_path or not os.path.isfile(self.db_path):
            logger.info(f"Database not found: {self.db_path}")
            return results
        
        try:
            conn = sqlite3.connect(self.db_path)
            
            # Single-doc results
            try:
                cur = conn.execute("""
                    SELECT doc_id, model, 
                           COUNT(*) as row_count,
                           MIN(timestamp) as first_ts,
                           MAX(timestamp) as last_ts
                    FROM single_doc_results
                    GROUP BY doc_id, model
                """)
                for row in cur.fetchall():
                    doc_id, model, count, first_ts, last_ts = row
                    first_dt = parse_iso_ts(first_ts)
                    last_dt = parse_iso_ts(last_ts)
                    duration = 0.0
                    if first_dt and last_dt:
                        duration = (last_dt - first_dt).total_seconds()
                    
                    key = f"single:{model}:{doc_id}"
                    results[key] = DbRunResult(
                        doc_id=doc_id,
                        model=model,
                        eval_type="single",
                        row_count=count or 0,
                        first_timestamp=first_dt,
                        last_timestamp=last_dt,
                        duration_seconds=duration,
                    )
            except Exception as e:
                logger.warning(f"Failed to query single_doc_results: {e}")
            
            # Pairwise results
            try:
                cur = conn.execute("""
                    SELECT doc_id_1, doc_id_2, model,
                           COUNT(*) as row_count,
                           MIN(timestamp) as first_ts,
                           MAX(timestamp) as last_ts
                    FROM pairwise_results
                    GROUP BY doc_id_1, doc_id_2, model
                """)
                for row in cur.fetchall():
                    doc1, doc2, model, count, first_ts, last_ts = row
                    first_dt = parse_iso_ts(first_ts)
                    last_dt = parse_iso_ts(last_ts)
                    duration = 0.0
                    if first_dt and last_dt:
                        duration = (last_dt - first_dt).total_seconds()
                    
                    key = f"pairwise:{model}:{doc1}:{doc2}"
                    results[key] = DbRunResult(
                        doc_id=doc1,
                        doc_id_2=doc2,
                        model=model,
                        eval_type="pairwise",
                        row_count=count or 0,
                        first_timestamp=first_dt,
                        last_timestamp=last_dt,
                        duration_seconds=duration,
                    )
            except Exception as e:
                logger.warning(f"Failed to query pairwise_results: {e}")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"Failed to connect to database: {e}")
        
        logger.info(f"Queried {len(results)} DB result entries")
        return results
    
    # -------------------------------------------------------------------------
    # CSV Fallback (SPEC 4)
    # -------------------------------------------------------------------------
    def load_csv_fallback(self) -> Dict[str, CsvRunData]:
        """
        Load CSV data as fallback (SPEC 4).
        
        Used when database is unavailable or for archived runs.
        
        Returns:
            Dict mapping key -> CsvRunData
        """
        results: Dict[str, CsvRunData] = {}
        
        if not self.csv_export_dir or not os.path.isdir(self.csv_export_dir):
            logger.info(f"CSV export directory not found: {self.csv_export_dir}")
            return results
        
        import csv
        
        for filename in os.listdir(self.csv_export_dir):
            if not filename.endswith(".csv"):
                continue
            
            filepath = os.path.join(self.csv_export_dir, filename)
            eval_type = "single" if "single_doc" in filename else "pairwise" if "pairwise" in filename else None
            
            if not eval_type:
                continue
            
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        doc_id = row.get("doc_id", "")
                        model = row.get("model", "")
                        if not doc_id or not model:
                            continue
                        
                        key = f"{eval_type}:{model}:{doc_id}"
                        if key not in results:
                            results[key] = CsvRunData(
                                doc_id=doc_id,
                                model=model,
                                eval_type=eval_type,
                                row_count=0,
                                timestamp=row.get("timestamp"),
                                source_file=filename,
                            )
                        results[key].row_count += 1
                        
            except Exception as e:
                logger.warning(f"Failed to parse CSV {filepath}: {e}")
        
        logger.info(f"Loaded {len(results)} CSV entries")
        return results
    
    # -------------------------------------------------------------------------
    # Matching Logic
    # -------------------------------------------------------------------------
    def match_expected_to_actual(
        self,
        expected: List[ExpectedRun],
        actual_logs: Dict[str, ActualRunLog],
        db_results: Dict[str, DbRunResult],
        csv_data: Optional[Dict[str, CsvRunData]] = None,
    ) -> Tuple[List[TimelineRow], List[Dict[str, Any]]]:
        """
        Match expected runs to actual execution data.
        
        Uses tiered matching:
        1. Exact: (phase, judge_model, target)
        2. Ordinal: (phase, judge_model, ordinal_index)
        3. Partial: (phase only) - marks as partial match
        
        Returns:
            Tuple of (matched_rows, unmatched_actuals)
        """
        rows: List[TimelineRow] = []
        used_log_keys: set = set()
        used_db_keys: set = set()
        
        # Build lookup indices for logs
        log_by_phase_judge_target: Dict[str, ActualRunLog] = {}
        log_by_phase_judge_ordinal: Dict[str, List[ActualRunLog]] = {}
        
        for log in actual_logs.values():
            if log.detected_phase and log.detected_target:
                key = f"{log.detected_phase.value}:{log.model}:{log.detected_target}"
                log_by_phase_judge_target[key] = log
            
            if log.detected_phase:
                ordinal_key = f"{log.detected_phase.value}:{log.model}"
                if ordinal_key not in log_by_phase_judge_ordinal:
                    log_by_phase_judge_ordinal[ordinal_key] = []
                log_by_phase_judge_ordinal[ordinal_key].append(log)
        
        # Sort ordinal lists by started_at for index matching
        for key in log_by_phase_judge_ordinal:
            log_by_phase_judge_ordinal[key].sort(key=lambda x: x.started_at)
        
        for exp in expected:
            row = TimelineRow(
                run_num=exp.run_num,
                phase=exp.phase.value,
                phase_display=exp.phase.display_name,
                phase_short=exp.phase.short_name,
                judge_model=exp.judge_model,
                target=exp.target,
                target_short=truncate_target(exp.target),
                expected_id=exp.expected_id,
                expected_index=exp.expected_index,
            )
            
            # Try to match log
            matched_log: Optional[ActualRunLog] = None
            match_status = MatchStatus.MISSING
            
            # Tier 1: Exact match
            exact_key = f"{exp.phase.value}:{exp.judge_model}:{exp.target}"
            if exact_key in log_by_phase_judge_target:
                matched_log = log_by_phase_judge_target[exact_key]
                match_status = MatchStatus.EXACT
            
            # Tier 2: Ordinal match
            if not matched_log:
                ordinal_key = f"{exp.phase.value}:{exp.judge_model}"
                ordinal_list = log_by_phase_judge_ordinal.get(ordinal_key, [])
                if exp.expected_index <= len(ordinal_list):
                    matched_log = ordinal_list[exp.expected_index - 1]
                    match_status = MatchStatus.ORDINAL
            
            # Tier 3: Partial (phase only) - disabled for now to avoid false matches
            # Can be enabled if needed for debugging
            
            if matched_log:
                used_log_keys.add(matched_log.dedup_key)
                row.log_start = matched_log.started_at.isoformat() if matched_log.started_at != datetime.min else None
                row.log_end = matched_log.finished_at.isoformat() if matched_log.finished_at != datetime.min else None
                row.log_duration_seconds = matched_log.duration_seconds
                row.log_duration_display = format_duration(matched_log.duration_seconds)
                row.prompt_tokens = matched_log.prompt_tokens
                row.completion_tokens = matched_log.completion_tokens
                row.total_tokens = matched_log.total_tokens
                row.total_cost_usd = matched_log.total_cost_usd
                row.cost_display = format_cost(matched_log.total_cost_usd)
                row.matched = True
                row.match_status = match_status.value
                row.source_used = SourceType.LOGS.value
                row.status_icon = "✓" if not matched_log.is_failure else "⚠"
            
            # Try to match DB result (supplements log data)
            # Extract model name from judge identifier (e.g., "google_gemini-2.5-flash" -> "gemini-2.5-flash")
            model_name = exp.judge_model.split("_", 1)[-1] if "_" in exp.judge_model else exp.judge_model
            
            if exp.phase in (EvalPhase.PRECOMBINE_SINGLE, EvalPhase.POSTCOMBINE_SINGLE):
                db_key = f"single:{model_name}:{exp.target}"
                db_result = db_results.get(db_key)
                if db_result:
                    used_db_keys.add(db_key)
                    row.db_row_count = db_result.row_count
                    row.db_first_ts = db_result.first_timestamp.isoformat() if db_result.first_timestamp else None
                    row.db_last_ts = db_result.last_timestamp.isoformat() if db_result.last_timestamp else None
                    row.db_duration_seconds = db_result.duration_seconds
                    row.db_duration_display = format_duration(db_result.duration_seconds)
                    
                    if not matched_log:
                        row.source_used = SourceType.SQLITE.value
                        row.matched = True
                        row.match_status = MatchStatus.PARTIAL.value
                        row.status_icon = "⚠"
            
            # Calculate delta
            if row.log_duration_seconds is not None and row.db_duration_seconds is not None:
                row.delta_seconds = row.log_duration_seconds - row.db_duration_seconds
                sign = "+" if row.delta_seconds >= 0 else ""
                row.delta_display = f"{sign}{row.delta_seconds:.1f}s"
            
            if not row.matched:
                row.status_icon = "✗"
                row.source_used = SourceType.CONFIG_ONLY.value
            
            rows.append(row)
        
        # Find unmatched actuals (per 4th party review)
        unmatched: List[Dict[str, Any]] = []
        for log in actual_logs.values():
            if log.dedup_key not in used_log_keys:
                unmatched.append({
                    "run_id": log.run_id,
                    "model": log.model,
                    "started_at": log.started_at.isoformat() if log.started_at != datetime.min else None,
                    "duration_seconds": log.duration_seconds,
                    "total_cost_usd": log.total_cost_usd,
                    "total_tokens": log.total_tokens,
                    "is_failure": log.is_failure,
                    "detected_phase": log.detected_phase.value if log.detected_phase else None,
                })
        
        return rows, unmatched
    
    # -------------------------------------------------------------------------
    # Subtotal Calculation
    # -------------------------------------------------------------------------
    def calculate_subtotals(self, rows: List[TimelineRow]) -> List[PhaseSubtotal]:
        """Calculate subtotals by phase."""
        subtotals: List[PhaseSubtotal] = []
        phase_rows: Dict[str, List[TimelineRow]] = {}
        
        # Group by phase
        for row in rows:
            if row.phase not in phase_rows:
                phase_rows[row.phase] = []
            phase_rows[row.phase].append(row)
        
        # Calculate per phase
        for phase_value in EvalPhase:
            phase_key = phase_value.value
            if phase_key not in phase_rows:
                continue
            
            p_rows = phase_rows[phase_key]
            subtotal = PhaseSubtotal(
                phase=phase_key,
                phase_display=phase_value.display_name,
                run_count=len(p_rows),
                expected_count=len(p_rows),
                matched_count=sum(1 for r in p_rows if r.matched),
                total_duration_seconds=sum(r.log_duration_seconds or 0 for r in p_rows),
                total_prompt_tokens=sum(r.prompt_tokens for r in p_rows),
                total_completion_tokens=sum(r.completion_tokens for r in p_rows),
                total_tokens=sum(r.total_tokens for r in p_rows),
                total_cost_usd=sum(r.total_cost_usd for r in p_rows),
                total_db_rows=sum(r.db_row_count for r in p_rows),
            )
            subtotal.total_duration_display = format_duration(subtotal.total_duration_seconds)
            subtotal.cost_display = format_cost(subtotal.total_cost_usd)
            subtotals.append(subtotal)
        
        return subtotals
    
    def _calculate_grand_total(self, subtotals: List[PhaseSubtotal]) -> PhaseSubtotal:
        """Calculate grand total from phase subtotals."""
        grand = PhaseSubtotal(
            phase="grand_total",
            phase_display="Grand Total",
            run_count=sum(s.run_count for s in subtotals),
            expected_count=sum(s.expected_count for s in subtotals),
            matched_count=sum(s.matched_count for s in subtotals),
            total_duration_seconds=sum(s.total_duration_seconds for s in subtotals),
            total_prompt_tokens=sum(s.total_prompt_tokens for s in subtotals),
            total_completion_tokens=sum(s.total_completion_tokens for s in subtotals),
            total_tokens=sum(s.total_tokens for s in subtotals),
            total_cost_usd=sum(s.total_cost_usd for s in subtotals),
            total_db_rows=sum(s.total_db_rows for s in subtotals),
        )
        grand.total_duration_display = format_duration(grand.total_duration_seconds)
        grand.cost_display = format_cost(grand.total_cost_usd)
        return grand
    
    # -------------------------------------------------------------------------
    # Main Generation
    # -------------------------------------------------------------------------
    def generate_chart(self) -> TimelineChart:
        """
        Generate complete timeline chart.
        
        This is the main entry point. Combines all data sources
        and produces a TimelineChart with rows, subtotals, and metadata.
        """
        logger.info("Generating evaluation timeline chart...")
        
        # Generate expected runs from config
        expected = self.generate_expected_runs()
        
        # Collect actual data from all sources
        actual_logs = self.parse_fpf_logs()
        db_results = self.query_db_results()
        csv_data = self.load_csv_fallback() if not db_results else {}
        
        # Use CSV as fallback if no DB results
        if not db_results and csv_data:
            # Convert CsvRunData to DbRunResult-like for matching
            for key, csv_entry in csv_data.items():
                db_results[key] = DbRunResult(
                    doc_id=csv_entry.doc_id,
                    model=csv_entry.model,
                    eval_type=csv_entry.eval_type,
                    row_count=csv_entry.row_count,
                    first_timestamp=parse_iso_ts(csv_entry.timestamp),
                    last_timestamp=parse_iso_ts(csv_entry.timestamp),
                )
        
        # Match expected to actual
        rows, unplanned_actuals = self.match_expected_to_actual(
            expected, actual_logs, db_results, csv_data
        )
        
        # Calculate subtotals
        subtotals = self.calculate_subtotals(rows)
        grand_total = self._calculate_grand_total(subtotals)
        
        # Determine overall run window
        run_start = None
        run_end = None
        for row in rows:
            if row.log_start:
                if run_start is None or row.log_start < run_start:
                    run_start = row.log_start
            if row.log_end:
                if run_end is None or row.log_end > run_end:
                    run_end = row.log_end
        
        # Build config snapshot (per 4th party review)
        config = self._load_config()
        config_snapshot = {
            "eval.mode": self._get_eval_mode(),
            "eval.pairwise_top_n": self._get_pairwise_top_n(),
            "combine.enabled": self._is_combine_enabled(),
            "combiner_limit": self._get_combiner_limit(),
            "judge_count": len(self._get_judge_models()),
            "doc_count": len(self._get_generated_docs()),
        }
        
        chart = TimelineChart(
            rows=rows,
            phase_subtotals=subtotals,
            grand_total=grand_total,
            unplanned_actuals=unplanned_actuals,
            run_start=run_start,
            run_end=run_end,
            total_run_count=len(rows),
            total_matched_count=sum(1 for r in rows if r.matched),
            config_snapshot=config_snapshot,
            sources={
                "config_path": self.config_path or "",
                "eval_config_path": self.eval_config_path or "",
                "db_path": self.db_path or "",
                "fpf_logs_dir": self.fpf_logs_dir or "",
                "csv_export_dir": self.csv_export_dir or "",
            },
            generated_at=datetime.now().isoformat(),
        )
        
        logger.info(
            f"Generated timeline chart: {chart.total_run_count} runs, "
            f"{chart.total_matched_count} matched, "
            f"{len(unplanned_actuals)} unplanned"
        )
        
        return chart
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Generate chart and convert to JSON-serializable dict.
        
        Use this method when passing to html_exporter to avoid
        circular dependencies from passing class instances.
        """
        chart = self.generate_chart()
        return asdict(chart)


# -----------------------------------------------------------------------------
# CLI Entry Point
# -----------------------------------------------------------------------------
def main():
    """CLI entry point for standalone testing."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Generate evaluation timeline chart from multiple sources."
    )
    parser.add_argument(
        "--config", required=True,
        help="Path to main config.yaml"
    )
    parser.add_argument(
        "--eval-config", required=True,
        help="Path to llm-doc-eval/config.yaml"
    )
    parser.add_argument(
        "--db", default=None,
        help="Path to SQLite database (optional)"
    )
    parser.add_argument(
        "--fpf-logs", default=None,
        help="Path to FPF logs directory (optional)"
    )
    parser.add_argument(
        "--csv-export", default=None,
        help="Path to CSV export directory (optional)"
    )
    parser.add_argument(
        "--output", default=None,
        help="Output JSON file path (default: stdout)"
    )
    parser.add_argument(
        "--time-start", default=None,
        help="Time window start (ISO timestamp)"
    )
    parser.add_argument(
        "--time-end", default=None,
        help="Time window end (ISO timestamp)"
    )
    
    args = parser.parse_args()
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(levelname)s: %(message)s"
    )
    
    try:
        aggregator = EvalTimelineAggregator(
            config_path=args.config,
            eval_config_path=args.eval_config,
            db_path=args.db,
            fpf_logs_dir=args.fpf_logs,
            csv_export_dir=args.csv_export,
            time_window_start=args.time_start,
            time_window_end=args.time_end,
        )
        
        chart_dict = aggregator.to_dict()
        
        if args.output:
            with open(args.output, "w", encoding="utf-8") as f:
                json.dump(chart_dict, f, indent=2)
            print(f"Timeline chart exported to: {args.output}")
        else:
            print(json.dumps(chart_dict, indent=2))
            
    except Exception as e:
        logger.error(f"Failed to generate timeline chart: {e}")
        raise


if __name__ == "__main__":
    main()
